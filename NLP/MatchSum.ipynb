{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# MatchSum Code\n",
    "* pytorch를 이용하여 구현\n",
    "* 핵심 : 좋은 요약은 좋지 않은 요약에 비해서 의미적으로 원본 text와 더 유사함\n",
    "* 흐름 : 요약된 candidate text선정 - 원본 text와 candidate 사이의 유사도 계산 모델 생성 - margin-based triplet loss를 이용하여 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "from transformers import BertModel, RobertaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "\n",
    "get candidate\n",
    "\n",
    "1. 각 text에서 sentence score가 높은(전체 text와 유사도가 큰) 문장을 5개 뽑아냄\n",
    "2. 뽑아낸 문장 5개 중 2~3를 조합하여 candidate text를 형성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MatchSum Model\n",
    "* encoder로 BERT, RoBERTa를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchSum(nn.Module):\n",
    "    \n",
    "    # init정의, encoder 입력이 무엇이냐에 따라 BERT 또는 RoBERTa 사용 \n",
    "    def __init__(self, candidate_num, encoder, hidden_size=768):\n",
    "        super(MatchSum, self).__init__() #생성자 호출\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.candidate_num  = candidate_num\n",
    "        \n",
    "        if encoder == 'bert':\n",
    "            self.encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "        else:\n",
    "            self.encoder = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "    def forward(self, text_id, candidate_id, summary_id):\n",
    "        # text_id는 원본 text 데이터\n",
    "        # candidate_id는 후보 요약본 데이터\n",
    "        # summary_id는 잘 요약된 데이터\n",
    "        \n",
    "        #배치 사이즈 결정\n",
    "        batch_size = text_id.size(0)\n",
    "        \n",
    "        pad_id = 0     # for BERT, <pad> token 0으로, masking\n",
    "        if text_id[0][0] == 0:\n",
    "            pad_id = 1 # for RoBERTa, masking x??\n",
    "\n",
    "        # get document embedding\n",
    "        input_mask = ~(text_id == pad_id)\n",
    "        out = self.encoder(text_id, attention_mask=input_mask)[0] # last layer\n",
    "        #1 for tokens that are not masked,\n",
    "        #0 for tokens that are masked.\n",
    "        \n",
    "        doc_emb = out[:, 0, :]\n",
    "        assert doc_emb.size() == (batch_size, self.hidden_size) # [batch_size, hidden_size], doc_emb size확인\n",
    "        \n",
    "        # get summary embedding\n",
    "        input_mask = ~(summary_id == pad_id)\n",
    "        out = self.encoder(summary_id, attention_mask=input_mask)[0] # last layer\n",
    "        summary_emb = out[:, 0, :]\n",
    "        assert summary_emb.size() == (batch_size, self.hidden_size) # [batch_size, hidden_size]\n",
    "\n",
    "        # get summary score\n",
    "        summary_score = torch.cosine_similarity(summary_emb, doc_emb, dim=-1) #실제 요약본과 원본 문서의 cosine similarity 계산\n",
    "\n",
    "        # get candidate embedding\n",
    "        candidate_num = candidate_id.size(1) #후보수를 candidate_id 사이즈로 지정\n",
    "        candidate_id = candidate_id.view(-1, candidate_id.size(-1)) #candidate_id reshape\n",
    "        input_mask = ~(candidate_id == pad_id)\n",
    "        out = self.encoder(candidate_id, attention_mask=input_mask)[0]\n",
    "        candidate_emb = out[:, 0, :].view(batch_size, candidate_num, self.hidden_size)  # [batch_size, candidate_num, hidden_size]\n",
    "        assert candidate_emb.size() == (batch_size, candidate_num, self.hidden_size)\n",
    "        \n",
    "        # get candidate score\n",
    "        doc_emb = doc_emb.unsqueeze(1).expand_as(candidate_emb)\n",
    "        score = torch.cosine_similarity(candidate_emb, doc_emb, dim=-1) # [batch_size, candidate_num]\n",
    "        assert score.size() == (batch_size, candidate_num)\n",
    "\n",
    "        return {'score': score, 'summary_score': summary_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "D : 원본 문서<br>\n",
    "C : 후보 요약문<br>\n",
    "C* : 정답 요약문<br>\n",
    "\n",
    "L1 = max(0, f(D, C) - f(D, C\\*) + r1)<br>\n",
    "L2 = max(0, f(D, C_j) - f(D, C_i) + (j-i)*r2), (i < j)\n",
    "\n",
    "margin-based triplet loss L = L1 + L2<br>\n",
    "=> 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "논문 저자 깃헙 : https://github.com/maszhongming/MatchSum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
