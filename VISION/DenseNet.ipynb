{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4dwPLc6ssYC"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8lKb9IcnjXz"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "1e54d995502743a9a408c72edabec8a2",
      "5df2b2cab2b247cf9287e90355bcd2fd",
      "ae2fb781a0104186b4c35c31ccf2aae6",
      "b6ea62a6adc040ae84a30649407d9e54",
      "b06c87ca860b41628522f9f3d829ece3",
      "4a8422957ef34c1f9405d311b48f6217",
      "98782d9738814ce18207039b233f8e53",
      "ab61a4b585334603860d89cdad0c274f"
     ]
    },
    "id": "0lDMD8VInpwZ",
    "outputId": "befe5bc8-40df-486a-8f18-1a8a8892f4ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e54d995502743a9a408c72edabec8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/cifar-10-python.tar.gz to ../data\n"
     ]
    }
   ],
   "source": [
    "# 배치 사이즈, learning rate, 총 layer 수를 파라미터로 설정 해놓음\n",
    "batch_size=64\n",
    "learning_rate = 0.1\n",
    "layers = 100\n",
    "\n",
    "# Train, Test set 받아오기\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data',train=True,download=True,transform=transform_train),\n",
    "    batch_size=batch_size,shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data',train=False,transform=transform_test),\n",
    "    batch_size=batch_size,shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0tUkLrgnumJ"
   },
   "outputs": [],
   "source": [
    "# Basic\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self,in_planes,out_planes,dropRate = 0.0):      \n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride = 1, padding = 1, bias = False)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.droprate>0:\n",
    "            out = F.dropout (out,p=self.droprate,training = self.training)\n",
    "        return torch.cat([x,out],1)\n",
    "        \n",
    "\n",
    "# Bottleneck Layer : ResNet, Inception처럼 bottleneck 구조를 사용함\n",
    "# 3x3 convolution 전에 1x1 convolution을 거쳐서 입력 feature map의 channel 개수를 줄이는 것 까지는 같은데\n",
    "# 그 뒤로 다시 입력 feature map의 channel 개수 만큼을 생성하는 대신 growth rate 만큼의 feature map을 생성하는 것이 차이 점\n",
    "# -> Computational cost 줄이기 가능\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self,in_planes,out_planes,dropRate=0.0):\n",
    "        super(BottleneckBlock,self).__init__()\n",
    "        inter_planes = out_planes * 4\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.conv1 = nn.Conv2d(in_planes,inter_planes,kernel_size=1,stride=1,padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(inter_planes)\n",
    "        self.conv2 = nn.Conv2d(inter_planes,out_planes,kernel_size=3,stride=1,padding=0,bias=False)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.droprate>0:\n",
    "            out = F.dropout(out,p=self.droprate,inplace=False,training = self.training)\n",
    "        out = self.conv2(self.relu(self.bn2(out)))\n",
    "        if self.droprate>0:\n",
    "            out = F.dropout(out,p=self.droprate,inplace=False,training = self.training)\n",
    "        return torch.cat([x,out],1) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXJPmcI9n29Y"
   },
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self,nb_layers,in_planes,growh_rate,block,dropRate=0.0):\n",
    "        super(DenseBlock,self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, growh_rate, nb_layers, dropRate)\n",
    "    \n",
    "    def _make_layer(self,block,in_planes,growh_rate,nb_layers,dropRate):\n",
    "        layers=[]\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(in_planes + i*growh_rate ,growh_rate,dropRate))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wd2KGubin5PC"
   },
   "outputs": [],
   "source": [
    "class TransitionBlock(nn.Module):\n",
    "    def __init__(self,in_planes,out_planes,dropRate=0.0):\n",
    "        super(TransitionBlock,self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes,out_planes,kernel_size=1,stride=1,padding=0,bias=False)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.droprate>0:\n",
    "            out = F.dropout(out,p=self.droprate,inplace=False,training=self.training)\n",
    "        return F.avg_pool2d(out,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8u6AauHen-p3"
   },
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self,depth,num_classes,growh_rate=12,reduction=0.5,bottleneck=True,dropRate=0.0):\n",
    "        super(DenseNet,self).__init__()\n",
    "        num_of_blocks = 3\n",
    "        in_planes = 16 \n",
    "        n = (depth - num_of_blocks - 1)/num_of_blocks # 총 depth에서 첫 conv , 2개의 transit , 마지막 linear 빼고 / num_of_blocks\n",
    "        if reduction != 1 :\n",
    "            in_planes = 2 * growh_rate\n",
    "        if bottleneck == True:\n",
    "            in_planes = 2 * growh_rate \n",
    "            n = n/2 \n",
    "            block = BottleneckBlock \n",
    "        else :\n",
    "            block = BasicBlock\n",
    "        \n",
    "        n = int(n) \n",
    "        self.conv1 = nn.Conv2d(3,in_planes,kernel_size=3,stride=1,padding=1,bias=False) \n",
    "        \n",
    "        \n",
    "        #1st block\n",
    "        \n",
    "        self.block1 = DenseBlock(n,in_planes,growh_rate,block,dropRate)\n",
    "        in_planes = int(in_planes+n*growh_rate) \n",
    "        \n",
    "        # in_planes,out_planes,dropRate\n",
    "        self.trans1 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)),dropRate=dropRate)\n",
    "        in_planes = int(math.floor(in_planes*reduction))\n",
    "        \n",
    "        \n",
    "        #2nd block\n",
    "        # nb_layers,in_planes,growh_rate,block,dropRate\n",
    "        self.block2 = DenseBlock(n,in_planes,growh_rate,block,dropRate)\n",
    "        in_planes = int(in_planes+n*growh_rate)\n",
    "        \n",
    "        # in_planes,out_planes,dropRate\n",
    "        self.trans2 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)),dropRate=dropRate)\n",
    "        in_planes = int(math.floor(in_planes*reduction))\n",
    "        \n",
    "        \n",
    "        #3rd block\n",
    "        # nb_layers,in_planes,growh_rate,block,dropRate\n",
    "        self.block3 = DenseBlock(n,in_planes,growh_rate,block,dropRate)\n",
    "        in_planes = int(in_planes+n*growh_rate)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        \n",
    "        self.fc = nn.Linear(in_planes,num_classes)\n",
    "        \n",
    "        self.in_planes = in_planes\n",
    "        \n",
    "        # module 초기화\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                \n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1) # \n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #x : 32*32\n",
    "        out = self.conv1(x) # 32*32\n",
    "        out = self.block1(out) # 32*32\n",
    "        out = self.trans1(out) # 16*16\n",
    "        out = self.block2(out) # 16*16\n",
    "        out = self.trans2(out) # 8*8\n",
    "        out = self.block3(out) # 8*8\n",
    "        out = self.relu(self.bn1(out)) #8*8\n",
    "        out = F.avg_pool2d(out,8) #1*1\n",
    "        out = out.view(-1, self.in_planes)\n",
    "        return self.fc(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pNp-y-_ToDTZ",
    "outputId": "fd23b8af-b97c-4da9-8e00-1d1298487e89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 769162\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# depth,num_classes <- cifar '10' ,growh_rate=12,reduction=0.5,bottleneck=True,dropRate=0.0\n",
    "\n",
    "\n",
    "#model = torch.load('DenseNetModelSave.pt')\n",
    "model = DenseNet(layers,10,growh_rate=12,dropRate = 0.0)\n",
    "\n",
    "\n",
    "# get the number of model parameters\n",
    "\n",
    "print('Number of model parameters: {}'.format(\n",
    "    sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate,\n",
    "                            momentum=0.9,nesterov=True,weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B31a_qN-oFiJ"
   },
   "outputs": [],
   "source": [
    "def train(train_loader,model,criterion,optimizer,epoch):\n",
    "    model.train()\n",
    "    for i, (input,target) in enumerate(train_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "        \n",
    "        output = model(input)\n",
    "        loss = criterion(output,target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(i%20 == 0):\n",
    "            print(\"loss in epoch %d , step %d : %f\" % (epoch, i,loss.data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAxHd2UWoHs2"
   },
   "outputs": [],
   "source": [
    "def test(test_loader,model,criterion,epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    \n",
    "    for i, (input,target) in enumerate(test_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "        \n",
    "        output = model(input)\n",
    "        loss = criterion(output,target)\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().float().sum()\n",
    "    \n",
    "    print(\"Accuracy in epoch %d : %f\" % (epoch,100.0*correct/len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTOI2f3IoKGz"
   },
   "outputs": [],
   "source": [
    "def adjust_lr(optimizer, epoch, learning_rate):\n",
    "    if epoch==15:\n",
    "        learning_rate*=0.1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gkz6JxrxoOdQ",
    "outputId": "6478ea5d-4c2f-4d20-b16a-3946f0bbc8ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in epoch 0 , step 0 : 2.304410\n",
      "loss in epoch 0 , step 20 : 1.905923\n",
      "loss in epoch 0 , step 40 : 2.070671\n",
      "loss in epoch 0 , step 60 : 1.868646\n",
      "loss in epoch 0 , step 80 : 1.808228\n",
      "loss in epoch 0 , step 100 : 1.658661\n",
      "loss in epoch 0 , step 120 : 1.531688\n",
      "loss in epoch 0 , step 140 : 1.671514\n",
      "loss in epoch 0 , step 160 : 1.429664\n",
      "loss in epoch 0 , step 180 : 1.788417\n",
      "loss in epoch 0 , step 200 : 1.362588\n",
      "loss in epoch 0 , step 220 : 1.624362\n",
      "loss in epoch 0 , step 240 : 1.582504\n",
      "loss in epoch 0 , step 260 : 1.437093\n",
      "loss in epoch 0 , step 280 : 1.611432\n",
      "loss in epoch 0 , step 300 : 1.347045\n",
      "loss in epoch 0 , step 320 : 1.378518\n",
      "loss in epoch 0 , step 340 : 1.646602\n",
      "loss in epoch 0 , step 360 : 1.180884\n",
      "loss in epoch 0 , step 380 : 1.486677\n",
      "loss in epoch 0 , step 400 : 1.282289\n",
      "loss in epoch 0 , step 420 : 1.376388\n",
      "loss in epoch 0 , step 440 : 1.471908\n",
      "loss in epoch 0 , step 460 : 1.159019\n",
      "loss in epoch 0 , step 480 : 1.431210\n",
      "loss in epoch 0 , step 500 : 1.058682\n",
      "loss in epoch 0 , step 520 : 1.146765\n",
      "loss in epoch 0 , step 540 : 1.119502\n",
      "loss in epoch 0 , step 560 : 1.143167\n",
      "loss in epoch 0 , step 580 : 1.050577\n",
      "loss in epoch 0 , step 600 : 1.142372\n",
      "loss in epoch 0 , step 620 : 1.078191\n",
      "loss in epoch 0 , step 640 : 0.855505\n",
      "loss in epoch 0 , step 660 : 1.113642\n",
      "loss in epoch 0 , step 680 : 1.005263\n",
      "loss in epoch 0 , step 700 : 0.841308\n",
      "loss in epoch 0 , step 720 : 0.950391\n",
      "loss in epoch 0 , step 740 : 1.168168\n",
      "loss in epoch 0 , step 760 : 1.242412\n",
      "loss in epoch 0 , step 780 : 1.069816\n",
      "Accuracy in epoch 0 : 60.720001\n",
      "loss in epoch 1 , step 0 : 1.002760\n",
      "loss in epoch 1 , step 20 : 0.931128\n",
      "loss in epoch 1 , step 40 : 1.018523\n",
      "loss in epoch 1 , step 60 : 0.885630\n",
      "loss in epoch 1 , step 80 : 1.123004\n",
      "loss in epoch 1 , step 100 : 0.814376\n",
      "loss in epoch 1 , step 120 : 1.238627\n",
      "loss in epoch 1 , step 140 : 0.814070\n",
      "loss in epoch 1 , step 160 : 1.103587\n",
      "loss in epoch 1 , step 180 : 0.836299\n",
      "loss in epoch 1 , step 200 : 0.804823\n",
      "loss in epoch 1 , step 220 : 0.985142\n",
      "loss in epoch 1 , step 240 : 0.826798\n",
      "loss in epoch 1 , step 260 : 0.911855\n",
      "loss in epoch 1 , step 280 : 0.816358\n",
      "loss in epoch 1 , step 300 : 0.887234\n",
      "loss in epoch 1 , step 320 : 0.773940\n",
      "loss in epoch 1 , step 340 : 0.881565\n",
      "loss in epoch 1 , step 360 : 1.016972\n",
      "loss in epoch 1 , step 380 : 0.863457\n",
      "loss in epoch 1 , step 400 : 0.745881\n",
      "loss in epoch 1 , step 420 : 0.938097\n",
      "loss in epoch 1 , step 440 : 1.235597\n",
      "loss in epoch 1 , step 460 : 0.921424\n",
      "loss in epoch 1 , step 480 : 1.038843\n",
      "loss in epoch 1 , step 500 : 0.816472\n",
      "loss in epoch 1 , step 520 : 0.882891\n",
      "loss in epoch 1 , step 540 : 0.615506\n",
      "loss in epoch 1 , step 560 : 0.704890\n",
      "loss in epoch 1 , step 580 : 0.886053\n",
      "loss in epoch 1 , step 600 : 0.631131\n",
      "loss in epoch 1 , step 620 : 0.822663\n",
      "loss in epoch 1 , step 640 : 0.818849\n",
      "loss in epoch 1 , step 660 : 0.705146\n",
      "loss in epoch 1 , step 680 : 0.802765\n",
      "loss in epoch 1 , step 700 : 0.937202\n",
      "loss in epoch 1 , step 720 : 0.525542\n",
      "loss in epoch 1 , step 740 : 0.684750\n",
      "loss in epoch 1 , step 760 : 0.889739\n",
      "loss in epoch 1 , step 780 : 0.984102\n",
      "Accuracy in epoch 1 : 69.769997\n",
      "loss in epoch 2 , step 0 : 0.826906\n",
      "loss in epoch 2 , step 20 : 0.747760\n",
      "loss in epoch 2 , step 40 : 0.912972\n",
      "loss in epoch 2 , step 60 : 0.918427\n",
      "loss in epoch 2 , step 80 : 0.770907\n",
      "loss in epoch 2 , step 100 : 0.590950\n",
      "loss in epoch 2 , step 120 : 0.563855\n",
      "loss in epoch 2 , step 140 : 0.831824\n",
      "loss in epoch 2 , step 160 : 0.856055\n",
      "loss in epoch 2 , step 180 : 0.684989\n",
      "loss in epoch 2 , step 200 : 0.671731\n",
      "loss in epoch 2 , step 220 : 0.792074\n",
      "loss in epoch 2 , step 240 : 0.804867\n",
      "loss in epoch 2 , step 260 : 0.699348\n",
      "loss in epoch 2 , step 280 : 0.711557\n",
      "loss in epoch 2 , step 300 : 0.597709\n",
      "loss in epoch 2 , step 320 : 0.772584\n",
      "loss in epoch 2 , step 340 : 0.665848\n",
      "loss in epoch 2 , step 360 : 0.869761\n",
      "loss in epoch 2 , step 380 : 0.485517\n",
      "loss in epoch 2 , step 400 : 0.927712\n",
      "loss in epoch 2 , step 420 : 0.713532\n",
      "loss in epoch 2 , step 440 : 0.867850\n",
      "loss in epoch 2 , step 460 : 0.584401\n",
      "loss in epoch 2 , step 480 : 0.686863\n",
      "loss in epoch 2 , step 500 : 0.553645\n",
      "loss in epoch 2 , step 520 : 0.606646\n",
      "loss in epoch 2 , step 540 : 0.689116\n",
      "loss in epoch 2 , step 560 : 0.515479\n",
      "loss in epoch 2 , step 580 : 0.623177\n",
      "loss in epoch 2 , step 600 : 0.677363\n",
      "loss in epoch 2 , step 620 : 0.740418\n",
      "loss in epoch 2 , step 640 : 0.764633\n",
      "loss in epoch 2 , step 660 : 0.581903\n",
      "loss in epoch 2 , step 680 : 0.528058\n",
      "loss in epoch 2 , step 700 : 0.514900\n",
      "loss in epoch 2 , step 720 : 0.837542\n",
      "loss in epoch 2 , step 740 : 0.483327\n",
      "loss in epoch 2 , step 760 : 0.582719\n",
      "loss in epoch 2 , step 780 : 0.521957\n",
      "Accuracy in epoch 2 : 73.209999\n",
      "loss in epoch 3 , step 0 : 0.521274\n",
      "loss in epoch 3 , step 20 : 0.676906\n",
      "loss in epoch 3 , step 40 : 0.470791\n",
      "loss in epoch 3 , step 60 : 0.616085\n",
      "loss in epoch 3 , step 80 : 0.813337\n",
      "loss in epoch 3 , step 100 : 0.631009\n",
      "loss in epoch 3 , step 120 : 0.526354\n",
      "loss in epoch 3 , step 140 : 0.566518\n",
      "loss in epoch 3 , step 160 : 0.751019\n",
      "loss in epoch 3 , step 180 : 0.453775\n",
      "loss in epoch 3 , step 200 : 0.442763\n",
      "loss in epoch 3 , step 220 : 0.557369\n",
      "loss in epoch 3 , step 240 : 0.470711\n",
      "loss in epoch 3 , step 260 : 0.401596\n",
      "loss in epoch 3 , step 280 : 0.816261\n",
      "loss in epoch 3 , step 300 : 0.590943\n",
      "loss in epoch 3 , step 320 : 0.802866\n",
      "loss in epoch 3 , step 340 : 0.359855\n",
      "loss in epoch 3 , step 360 : 0.578117\n",
      "loss in epoch 3 , step 380 : 0.864126\n",
      "loss in epoch 3 , step 400 : 0.423345\n",
      "loss in epoch 3 , step 420 : 0.597147\n",
      "loss in epoch 3 , step 440 : 0.639710\n",
      "loss in epoch 3 , step 460 : 0.521871\n",
      "loss in epoch 3 , step 480 : 0.505308\n",
      "loss in epoch 3 , step 500 : 0.447441\n",
      "loss in epoch 3 , step 520 : 0.463098\n",
      "loss in epoch 3 , step 540 : 0.474491\n",
      "loss in epoch 3 , step 560 : 0.453600\n",
      "loss in epoch 3 , step 580 : 0.583328\n",
      "loss in epoch 3 , step 600 : 0.639026\n",
      "loss in epoch 3 , step 620 : 0.520316\n",
      "loss in epoch 3 , step 640 : 0.526465\n",
      "loss in epoch 3 , step 660 : 1.004289\n",
      "loss in epoch 3 , step 680 : 0.550577\n",
      "loss in epoch 3 , step 700 : 0.736501\n",
      "loss in epoch 3 , step 720 : 0.792578\n",
      "loss in epoch 3 , step 740 : 0.405437\n",
      "loss in epoch 3 , step 760 : 0.488494\n",
      "loss in epoch 3 , step 780 : 0.493455\n",
      "Accuracy in epoch 3 : 76.360001\n",
      "loss in epoch 4 , step 0 : 0.666479\n",
      "loss in epoch 4 , step 20 : 0.542014\n",
      "loss in epoch 4 , step 40 : 0.589750\n",
      "loss in epoch 4 , step 60 : 0.656809\n",
      "loss in epoch 4 , step 80 : 0.560556\n",
      "loss in epoch 4 , step 100 : 0.424976\n",
      "loss in epoch 4 , step 120 : 0.422719\n",
      "loss in epoch 4 , step 140 : 0.539518\n",
      "loss in epoch 4 , step 160 : 0.538057\n",
      "loss in epoch 4 , step 180 : 0.385460\n",
      "loss in epoch 4 , step 200 : 0.572141\n",
      "loss in epoch 4 , step 220 : 0.848689\n",
      "loss in epoch 4 , step 240 : 0.634641\n",
      "loss in epoch 4 , step 260 : 0.440990\n",
      "loss in epoch 4 , step 280 : 0.594924\n",
      "loss in epoch 4 , step 300 : 0.486296\n",
      "loss in epoch 4 , step 320 : 0.470295\n",
      "loss in epoch 4 , step 340 : 0.523635\n",
      "loss in epoch 4 , step 360 : 0.560760\n",
      "loss in epoch 4 , step 380 : 0.734228\n",
      "loss in epoch 4 , step 400 : 0.680438\n",
      "loss in epoch 4 , step 420 : 0.557148\n",
      "loss in epoch 4 , step 440 : 0.432308\n",
      "loss in epoch 4 , step 460 : 0.555149\n",
      "loss in epoch 4 , step 480 : 0.332735\n",
      "loss in epoch 4 , step 500 : 0.566791\n",
      "loss in epoch 4 , step 520 : 0.466496\n",
      "loss in epoch 4 , step 540 : 0.501983\n",
      "loss in epoch 4 , step 560 : 0.517278\n",
      "loss in epoch 4 , step 580 : 0.331669\n",
      "loss in epoch 4 , step 600 : 0.479030\n",
      "loss in epoch 4 , step 620 : 0.581275\n",
      "loss in epoch 4 , step 640 : 0.517637\n",
      "loss in epoch 4 , step 660 : 0.543598\n",
      "loss in epoch 4 , step 680 : 0.536625\n",
      "loss in epoch 4 , step 700 : 0.606878\n",
      "loss in epoch 4 , step 720 : 0.474528\n",
      "loss in epoch 4 , step 740 : 0.437017\n",
      "loss in epoch 4 , step 760 : 0.369782\n",
      "loss in epoch 4 , step 780 : 0.654163\n",
      "Accuracy in epoch 4 : 74.080002\n",
      "loss in epoch 5 , step 0 : 0.272281\n",
      "loss in epoch 5 , step 20 : 0.598201\n",
      "loss in epoch 5 , step 40 : 0.536121\n",
      "loss in epoch 5 , step 60 : 0.454118\n",
      "loss in epoch 5 , step 80 : 0.624651\n",
      "loss in epoch 5 , step 100 : 0.538566\n",
      "loss in epoch 5 , step 120 : 0.477184\n",
      "loss in epoch 5 , step 140 : 0.402573\n",
      "loss in epoch 5 , step 160 : 0.634422\n",
      "loss in epoch 5 , step 180 : 0.515163\n",
      "loss in epoch 5 , step 200 : 0.439312\n",
      "loss in epoch 5 , step 220 : 0.547226\n",
      "loss in epoch 5 , step 240 : 0.554585\n",
      "loss in epoch 5 , step 260 : 0.388816\n",
      "loss in epoch 5 , step 280 : 0.412450\n",
      "loss in epoch 5 , step 300 : 0.451538\n",
      "loss in epoch 5 , step 320 : 0.573041\n",
      "loss in epoch 5 , step 340 : 0.553474\n",
      "loss in epoch 5 , step 360 : 0.544412\n",
      "loss in epoch 5 , step 380 : 0.504750\n",
      "loss in epoch 5 , step 400 : 0.285921\n",
      "loss in epoch 5 , step 420 : 0.417261\n",
      "loss in epoch 5 , step 440 : 0.460215\n",
      "loss in epoch 5 , step 460 : 0.598866\n",
      "loss in epoch 5 , step 480 : 0.579671\n",
      "loss in epoch 5 , step 500 : 0.678869\n",
      "loss in epoch 5 , step 520 : 0.510381\n",
      "loss in epoch 5 , step 540 : 0.631907\n",
      "loss in epoch 5 , step 560 : 0.477082\n",
      "loss in epoch 5 , step 580 : 0.308442\n",
      "loss in epoch 5 , step 600 : 0.354879\n",
      "loss in epoch 5 , step 620 : 0.396318\n",
      "loss in epoch 5 , step 640 : 0.388089\n",
      "loss in epoch 5 , step 660 : 0.513237\n",
      "loss in epoch 5 , step 680 : 0.575413\n",
      "loss in epoch 5 , step 700 : 0.481319\n",
      "loss in epoch 5 , step 720 : 0.488636\n",
      "loss in epoch 5 , step 740 : 0.542284\n",
      "loss in epoch 5 , step 760 : 0.531446\n",
      "loss in epoch 5 , step 780 : 0.590989\n",
      "Accuracy in epoch 5 : 79.620003\n",
      "loss in epoch 6 , step 0 : 0.781666\n",
      "loss in epoch 6 , step 20 : 0.428099\n",
      "loss in epoch 6 , step 40 : 0.559923\n",
      "loss in epoch 6 , step 60 : 0.439982\n",
      "loss in epoch 6 , step 80 : 0.351112\n",
      "loss in epoch 6 , step 100 : 0.460908\n",
      "loss in epoch 6 , step 120 : 0.713113\n",
      "loss in epoch 6 , step 140 : 0.486814\n",
      "loss in epoch 6 , step 160 : 0.525022\n",
      "loss in epoch 6 , step 180 : 0.338201\n",
      "loss in epoch 6 , step 200 : 0.310610\n",
      "loss in epoch 6 , step 220 : 0.291880\n",
      "loss in epoch 6 , step 240 : 0.237269\n",
      "loss in epoch 6 , step 260 : 0.756949\n",
      "loss in epoch 6 , step 280 : 0.541089\n",
      "loss in epoch 6 , step 300 : 0.392297\n",
      "loss in epoch 6 , step 320 : 0.392505\n",
      "loss in epoch 6 , step 340 : 0.334880\n",
      "loss in epoch 6 , step 360 : 0.556281\n",
      "loss in epoch 6 , step 380 : 0.361599\n",
      "loss in epoch 6 , step 400 : 0.377229\n",
      "loss in epoch 6 , step 420 : 0.375171\n",
      "loss in epoch 6 , step 440 : 0.519024\n",
      "loss in epoch 6 , step 460 : 0.289559\n",
      "loss in epoch 6 , step 480 : 0.354851\n",
      "loss in epoch 6 , step 500 : 0.617784\n",
      "loss in epoch 6 , step 520 : 0.340550\n",
      "loss in epoch 6 , step 540 : 0.444419\n",
      "loss in epoch 6 , step 560 : 0.624604\n",
      "loss in epoch 6 , step 580 : 0.491561\n",
      "loss in epoch 6 , step 600 : 0.321186\n",
      "loss in epoch 6 , step 620 : 0.296642\n",
      "loss in epoch 6 , step 640 : 0.262050\n",
      "loss in epoch 6 , step 660 : 0.324971\n",
      "loss in epoch 6 , step 680 : 0.601358\n",
      "loss in epoch 6 , step 700 : 0.471315\n",
      "loss in epoch 6 , step 720 : 0.462805\n",
      "loss in epoch 6 , step 740 : 0.376269\n",
      "loss in epoch 6 , step 760 : 0.342902\n",
      "loss in epoch 6 , step 780 : 0.465729\n",
      "Accuracy in epoch 6 : 80.989998\n",
      "loss in epoch 7 , step 0 : 0.329972\n",
      "loss in epoch 7 , step 20 : 0.473616\n",
      "loss in epoch 7 , step 40 : 0.440720\n",
      "loss in epoch 7 , step 60 : 0.517114\n",
      "loss in epoch 7 , step 80 : 0.484350\n",
      "loss in epoch 7 , step 100 : 0.386659\n",
      "loss in epoch 7 , step 120 : 0.349456\n",
      "loss in epoch 7 , step 140 : 0.399073\n",
      "loss in epoch 7 , step 160 : 0.367730\n",
      "loss in epoch 7 , step 180 : 0.319675\n",
      "loss in epoch 7 , step 200 : 0.581777\n",
      "loss in epoch 7 , step 220 : 0.589402\n",
      "loss in epoch 7 , step 240 : 0.464131\n",
      "loss in epoch 7 , step 260 : 0.391871\n",
      "loss in epoch 7 , step 280 : 0.327860\n",
      "loss in epoch 7 , step 300 : 0.418196\n",
      "loss in epoch 7 , step 320 : 0.640619\n",
      "loss in epoch 7 , step 340 : 0.633042\n",
      "loss in epoch 7 , step 360 : 0.543117\n",
      "loss in epoch 7 , step 380 : 0.446366\n",
      "loss in epoch 7 , step 400 : 0.325958\n",
      "loss in epoch 7 , step 420 : 0.406842\n",
      "loss in epoch 7 , step 440 : 0.334827\n",
      "loss in epoch 7 , step 460 : 0.628108\n",
      "loss in epoch 7 , step 480 : 0.397408\n",
      "loss in epoch 7 , step 500 : 0.442831\n",
      "loss in epoch 7 , step 520 : 0.405691\n",
      "loss in epoch 7 , step 540 : 0.543266\n",
      "loss in epoch 7 , step 560 : 0.390746\n",
      "loss in epoch 7 , step 580 : 0.319129\n",
      "loss in epoch 7 , step 600 : 0.565794\n",
      "loss in epoch 7 , step 620 : 0.419069\n",
      "loss in epoch 7 , step 640 : 0.390620\n",
      "loss in epoch 7 , step 660 : 0.415889\n",
      "loss in epoch 7 , step 680 : 0.250200\n",
      "loss in epoch 7 , step 700 : 0.301275\n",
      "loss in epoch 7 , step 720 : 0.259049\n",
      "loss in epoch 7 , step 740 : 0.364547\n",
      "loss in epoch 7 , step 760 : 0.356664\n",
      "loss in epoch 7 , step 780 : 0.249828\n",
      "Accuracy in epoch 7 : 80.260002\n",
      "loss in epoch 8 , step 0 : 0.443611\n",
      "loss in epoch 8 , step 20 : 0.602106\n",
      "loss in epoch 8 , step 40 : 0.371765\n",
      "loss in epoch 8 , step 60 : 0.703766\n",
      "loss in epoch 8 , step 80 : 0.251856\n",
      "loss in epoch 8 , step 100 : 0.367327\n",
      "loss in epoch 8 , step 120 : 0.329146\n",
      "loss in epoch 8 , step 140 : 0.315338\n",
      "loss in epoch 8 , step 160 : 0.421246\n",
      "loss in epoch 8 , step 180 : 0.537268\n",
      "loss in epoch 8 , step 200 : 0.320822\n",
      "loss in epoch 8 , step 220 : 0.306731\n",
      "loss in epoch 8 , step 240 : 0.441061\n",
      "loss in epoch 8 , step 260 : 0.394756\n",
      "loss in epoch 8 , step 280 : 0.514196\n",
      "loss in epoch 8 , step 300 : 0.301431\n",
      "loss in epoch 8 , step 320 : 0.419349\n",
      "loss in epoch 8 , step 340 : 0.438270\n",
      "loss in epoch 8 , step 360 : 0.502962\n",
      "loss in epoch 8 , step 380 : 0.644838\n",
      "loss in epoch 8 , step 400 : 0.436135\n",
      "loss in epoch 8 , step 420 : 0.362591\n",
      "loss in epoch 8 , step 440 : 0.496598\n",
      "loss in epoch 8 , step 460 : 0.239786\n",
      "loss in epoch 8 , step 480 : 0.174113\n",
      "loss in epoch 8 , step 500 : 0.279775\n",
      "loss in epoch 8 , step 520 : 0.408565\n",
      "loss in epoch 8 , step 540 : 0.465561\n",
      "loss in epoch 8 , step 560 : 0.481195\n",
      "loss in epoch 8 , step 580 : 0.487669\n",
      "loss in epoch 8 , step 600 : 0.299615\n",
      "loss in epoch 8 , step 620 : 0.434875\n",
      "loss in epoch 8 , step 640 : 0.282751\n",
      "loss in epoch 8 , step 660 : 0.581069\n",
      "loss in epoch 8 , step 680 : 0.429772\n",
      "loss in epoch 8 , step 700 : 0.353891\n",
      "loss in epoch 8 , step 720 : 0.346869\n",
      "loss in epoch 8 , step 740 : 0.497618\n",
      "loss in epoch 8 , step 760 : 0.499251\n",
      "loss in epoch 8 , step 780 : 0.323537\n",
      "Accuracy in epoch 8 : 81.669998\n",
      "loss in epoch 9 , step 0 : 0.326933\n",
      "loss in epoch 9 , step 20 : 0.305657\n",
      "loss in epoch 9 , step 40 : 0.463755\n",
      "loss in epoch 9 , step 60 : 0.284162\n",
      "loss in epoch 9 , step 80 : 0.307490\n",
      "loss in epoch 9 , step 100 : 0.302001\n",
      "loss in epoch 9 , step 120 : 0.322750\n",
      "loss in epoch 9 , step 140 : 0.422754\n",
      "loss in epoch 9 , step 160 : 0.346621\n",
      "loss in epoch 9 , step 180 : 0.520633\n",
      "loss in epoch 9 , step 200 : 0.243690\n",
      "loss in epoch 9 , step 220 : 0.305251\n",
      "loss in epoch 9 , step 240 : 0.328333\n",
      "loss in epoch 9 , step 260 : 0.409331\n",
      "loss in epoch 9 , step 280 : 0.388669\n",
      "loss in epoch 9 , step 300 : 0.352665\n",
      "loss in epoch 9 , step 320 : 0.449053\n",
      "loss in epoch 9 , step 340 : 0.452104\n",
      "loss in epoch 9 , step 360 : 0.587650\n",
      "loss in epoch 9 , step 380 : 0.408173\n",
      "loss in epoch 9 , step 400 : 0.529350\n",
      "loss in epoch 9 , step 420 : 0.305377\n",
      "loss in epoch 9 , step 440 : 0.535550\n",
      "loss in epoch 9 , step 460 : 0.437875\n",
      "loss in epoch 9 , step 480 : 0.259293\n",
      "loss in epoch 9 , step 500 : 0.240403\n",
      "loss in epoch 9 , step 520 : 0.623446\n",
      "loss in epoch 9 , step 540 : 0.281563\n",
      "loss in epoch 9 , step 560 : 0.521077\n",
      "loss in epoch 9 , step 580 : 0.238089\n",
      "loss in epoch 9 , step 600 : 0.525219\n",
      "loss in epoch 9 , step 620 : 0.266783\n",
      "loss in epoch 9 , step 640 : 0.597478\n",
      "loss in epoch 9 , step 660 : 0.227815\n",
      "loss in epoch 9 , step 680 : 0.466599\n",
      "loss in epoch 9 , step 700 : 0.287249\n",
      "loss in epoch 9 , step 720 : 0.328367\n",
      "loss in epoch 9 , step 740 : 0.287828\n",
      "loss in epoch 9 , step 760 : 0.381194\n",
      "loss in epoch 9 , step 780 : 0.255004\n",
      "Accuracy in epoch 9 : 85.320000\n",
      "loss in epoch 10 , step 0 : 0.428610\n",
      "loss in epoch 10 , step 20 : 0.540004\n",
      "loss in epoch 10 , step 40 : 0.359440\n",
      "loss in epoch 10 , step 60 : 0.394688\n",
      "loss in epoch 10 , step 80 : 0.299356\n",
      "loss in epoch 10 , step 100 : 0.369812\n",
      "loss in epoch 10 , step 120 : 0.206015\n",
      "loss in epoch 10 , step 140 : 0.522316\n",
      "loss in epoch 10 , step 160 : 0.361615\n",
      "loss in epoch 10 , step 180 : 0.591414\n",
      "loss in epoch 10 , step 200 : 0.315293\n",
      "loss in epoch 10 , step 220 : 0.355776\n",
      "loss in epoch 10 , step 240 : 0.529918\n",
      "loss in epoch 10 , step 260 : 0.319783\n",
      "loss in epoch 10 , step 280 : 0.516306\n",
      "loss in epoch 10 , step 300 : 0.468610\n",
      "loss in epoch 10 , step 320 : 0.276063\n",
      "loss in epoch 10 , step 340 : 0.470191\n",
      "loss in epoch 10 , step 360 : 0.287031\n",
      "loss in epoch 10 , step 380 : 0.217132\n",
      "loss in epoch 10 , step 400 : 0.341061\n",
      "loss in epoch 10 , step 420 : 0.445695\n",
      "loss in epoch 10 , step 440 : 0.363005\n",
      "loss in epoch 10 , step 460 : 0.481219\n",
      "loss in epoch 10 , step 480 : 0.525458\n",
      "loss in epoch 10 , step 500 : 0.598882\n",
      "loss in epoch 10 , step 520 : 0.228666\n",
      "loss in epoch 10 , step 540 : 0.538723\n",
      "loss in epoch 10 , step 560 : 0.453863\n",
      "loss in epoch 10 , step 580 : 0.334814\n",
      "loss in epoch 10 , step 600 : 0.415451\n",
      "loss in epoch 10 , step 620 : 0.469766\n",
      "loss in epoch 10 , step 640 : 0.193136\n",
      "loss in epoch 10 , step 660 : 0.367398\n",
      "loss in epoch 10 , step 680 : 0.367626\n",
      "loss in epoch 10 , step 700 : 0.255053\n",
      "loss in epoch 10 , step 720 : 0.330590\n",
      "loss in epoch 10 , step 740 : 0.372382\n",
      "loss in epoch 10 , step 760 : 0.284481\n",
      "loss in epoch 10 , step 780 : 0.257243\n",
      "Accuracy in epoch 10 : 82.500000\n",
      "loss in epoch 11 , step 0 : 0.393476\n",
      "loss in epoch 11 , step 20 : 0.189512\n",
      "loss in epoch 11 , step 40 : 0.278174\n",
      "loss in epoch 11 , step 60 : 0.345812\n",
      "loss in epoch 11 , step 80 : 0.236811\n",
      "loss in epoch 11 , step 100 : 0.310185\n",
      "loss in epoch 11 , step 120 : 0.473392\n",
      "loss in epoch 11 , step 140 : 0.314657\n",
      "loss in epoch 11 , step 160 : 0.384515\n",
      "loss in epoch 11 , step 180 : 0.364750\n",
      "loss in epoch 11 , step 200 : 0.295407\n",
      "loss in epoch 11 , step 220 : 0.313138\n",
      "loss in epoch 11 , step 240 : 0.334550\n",
      "loss in epoch 11 , step 260 : 0.529853\n",
      "loss in epoch 11 , step 280 : 0.415951\n",
      "loss in epoch 11 , step 300 : 0.290275\n",
      "loss in epoch 11 , step 320 : 0.337434\n",
      "loss in epoch 11 , step 340 : 0.449592\n",
      "loss in epoch 11 , step 360 : 0.459133\n",
      "loss in epoch 11 , step 380 : 0.252935\n",
      "loss in epoch 11 , step 400 : 0.423469\n",
      "loss in epoch 11 , step 420 : 0.445838\n",
      "loss in epoch 11 , step 440 : 0.514679\n",
      "loss in epoch 11 , step 460 : 0.388150\n",
      "loss in epoch 11 , step 480 : 0.528017\n",
      "loss in epoch 11 , step 500 : 0.275446\n",
      "loss in epoch 11 , step 520 : 0.241082\n",
      "loss in epoch 11 , step 540 : 0.339391\n",
      "loss in epoch 11 , step 560 : 0.621957\n",
      "loss in epoch 11 , step 580 : 0.295978\n",
      "loss in epoch 11 , step 600 : 0.250835\n",
      "loss in epoch 11 , step 620 : 0.318994\n",
      "loss in epoch 11 , step 640 : 0.314312\n",
      "loss in epoch 11 , step 660 : 0.338980\n",
      "loss in epoch 11 , step 680 : 0.310999\n",
      "loss in epoch 11 , step 700 : 0.339164\n",
      "loss in epoch 11 , step 720 : 0.313901\n",
      "loss in epoch 11 , step 740 : 0.432163\n",
      "loss in epoch 11 , step 760 : 0.457873\n",
      "loss in epoch 11 , step 780 : 0.288500\n",
      "Accuracy in epoch 11 : 83.769997\n",
      "loss in epoch 12 , step 0 : 0.280336\n",
      "loss in epoch 12 , step 20 : 0.397011\n",
      "loss in epoch 12 , step 40 : 0.346771\n",
      "loss in epoch 12 , step 60 : 0.322911\n",
      "loss in epoch 12 , step 80 : 0.239080\n",
      "loss in epoch 12 , step 100 : 0.411728\n",
      "loss in epoch 12 , step 120 : 0.407826\n",
      "loss in epoch 12 , step 140 : 0.417848\n",
      "loss in epoch 12 , step 160 : 0.292785\n",
      "loss in epoch 12 , step 180 : 0.256935\n",
      "loss in epoch 12 , step 200 : 0.353676\n",
      "loss in epoch 12 , step 220 : 0.264157\n",
      "loss in epoch 12 , step 240 : 0.304988\n",
      "loss in epoch 12 , step 260 : 0.476212\n",
      "loss in epoch 12 , step 280 : 0.354850\n",
      "loss in epoch 12 , step 300 : 0.288054\n",
      "loss in epoch 12 , step 320 : 0.389833\n",
      "loss in epoch 12 , step 340 : 0.320378\n",
      "loss in epoch 12 , step 360 : 0.450398\n",
      "loss in epoch 12 , step 380 : 0.364004\n",
      "loss in epoch 12 , step 400 : 0.362272\n",
      "loss in epoch 12 , step 420 : 0.299812\n",
      "loss in epoch 12 , step 440 : 0.214121\n",
      "loss in epoch 12 , step 460 : 0.455130\n",
      "loss in epoch 12 , step 480 : 0.476633\n",
      "loss in epoch 12 , step 500 : 0.195563\n",
      "loss in epoch 12 , step 520 : 0.322641\n",
      "loss in epoch 12 , step 540 : 0.333961\n",
      "loss in epoch 12 , step 560 : 0.302917\n",
      "loss in epoch 12 , step 580 : 0.411205\n",
      "loss in epoch 12 , step 600 : 0.418570\n",
      "loss in epoch 12 , step 620 : 0.332067\n",
      "loss in epoch 12 , step 640 : 0.250812\n",
      "loss in epoch 12 , step 660 : 0.259279\n",
      "loss in epoch 12 , step 680 : 0.279029\n",
      "loss in epoch 12 , step 700 : 0.213850\n",
      "loss in epoch 12 , step 720 : 0.283642\n",
      "loss in epoch 12 , step 740 : 0.243234\n",
      "loss in epoch 12 , step 760 : 0.357780\n",
      "loss in epoch 12 , step 780 : 0.535775\n",
      "Accuracy in epoch 12 : 84.650002\n",
      "loss in epoch 13 , step 0 : 0.373606\n",
      "loss in epoch 13 , step 20 : 0.328880\n",
      "loss in epoch 13 , step 40 : 0.484296\n",
      "loss in epoch 13 , step 60 : 0.344133\n",
      "loss in epoch 13 , step 80 : 0.501049\n",
      "loss in epoch 13 , step 100 : 0.362921\n",
      "loss in epoch 13 , step 120 : 0.503318\n",
      "loss in epoch 13 , step 140 : 0.534574\n",
      "loss in epoch 13 , step 160 : 0.541420\n",
      "loss in epoch 13 , step 180 : 0.440483\n",
      "loss in epoch 13 , step 200 : 0.268279\n",
      "loss in epoch 13 , step 220 : 0.338619\n",
      "loss in epoch 13 , step 240 : 0.348147\n",
      "loss in epoch 13 , step 260 : 0.413261\n",
      "loss in epoch 13 , step 280 : 0.379270\n",
      "loss in epoch 13 , step 300 : 0.261163\n",
      "loss in epoch 13 , step 320 : 0.417094\n",
      "loss in epoch 13 , step 340 : 0.272104\n",
      "loss in epoch 13 , step 360 : 0.431178\n",
      "loss in epoch 13 , step 380 : 0.322672\n",
      "loss in epoch 13 , step 400 : 0.325848\n",
      "loss in epoch 13 , step 420 : 0.288196\n",
      "loss in epoch 13 , step 440 : 0.255748\n",
      "loss in epoch 13 , step 460 : 0.291361\n",
      "loss in epoch 13 , step 480 : 0.617050\n",
      "loss in epoch 13 , step 500 : 0.418345\n",
      "loss in epoch 13 , step 520 : 0.277813\n",
      "loss in epoch 13 , step 540 : 0.523289\n",
      "loss in epoch 13 , step 560 : 0.263354\n",
      "loss in epoch 13 , step 580 : 0.441693\n",
      "loss in epoch 13 , step 600 : 0.367622\n",
      "loss in epoch 13 , step 620 : 0.377464\n",
      "loss in epoch 13 , step 640 : 0.456380\n",
      "loss in epoch 13 , step 660 : 0.277266\n",
      "loss in epoch 13 , step 680 : 0.328304\n",
      "loss in epoch 13 , step 700 : 0.259704\n",
      "loss in epoch 13 , step 720 : 0.188343\n",
      "loss in epoch 13 , step 740 : 0.257067\n",
      "loss in epoch 13 , step 760 : 0.497445\n",
      "loss in epoch 13 , step 780 : 0.203259\n",
      "Accuracy in epoch 13 : 83.550003\n",
      "loss in epoch 14 , step 0 : 0.458175\n",
      "loss in epoch 14 , step 20 : 0.194564\n",
      "loss in epoch 14 , step 40 : 0.406306\n",
      "loss in epoch 14 , step 60 : 0.405830\n",
      "loss in epoch 14 , step 80 : 0.155291\n",
      "loss in epoch 14 , step 100 : 0.200553\n",
      "loss in epoch 14 , step 120 : 0.183631\n",
      "loss in epoch 14 , step 140 : 0.248098\n",
      "loss in epoch 14 , step 160 : 0.221765\n",
      "loss in epoch 14 , step 180 : 0.268690\n",
      "loss in epoch 14 , step 200 : 0.346957\n",
      "loss in epoch 14 , step 220 : 0.297541\n",
      "loss in epoch 14 , step 240 : 0.358258\n",
      "loss in epoch 14 , step 260 : 0.460454\n",
      "loss in epoch 14 , step 280 : 0.318228\n",
      "loss in epoch 14 , step 300 : 0.457021\n",
      "loss in epoch 14 , step 320 : 0.228998\n",
      "loss in epoch 14 , step 340 : 0.390839\n",
      "loss in epoch 14 , step 360 : 0.386289\n",
      "loss in epoch 14 , step 380 : 0.478898\n",
      "loss in epoch 14 , step 400 : 0.242956\n",
      "loss in epoch 14 , step 420 : 0.163369\n",
      "loss in epoch 14 , step 440 : 0.405446\n",
      "loss in epoch 14 , step 460 : 0.366137\n",
      "loss in epoch 14 , step 480 : 0.210665\n",
      "loss in epoch 14 , step 500 : 0.260100\n",
      "loss in epoch 14 , step 520 : 0.360334\n",
      "loss in epoch 14 , step 540 : 0.405676\n",
      "loss in epoch 14 , step 560 : 0.349394\n",
      "loss in epoch 14 , step 580 : 0.502722\n",
      "loss in epoch 14 , step 600 : 0.351159\n",
      "loss in epoch 14 , step 620 : 0.456370\n",
      "loss in epoch 14 , step 640 : 0.356682\n",
      "loss in epoch 14 , step 660 : 0.338095\n",
      "loss in epoch 14 , step 680 : 0.346563\n",
      "loss in epoch 14 , step 700 : 0.215607\n",
      "loss in epoch 14 , step 720 : 0.373725\n",
      "loss in epoch 14 , step 740 : 0.437372\n",
      "loss in epoch 14 , step 760 : 0.386379\n",
      "loss in epoch 14 , step 780 : 0.303056\n",
      "Accuracy in epoch 14 : 85.110001\n",
      "loss in epoch 15 , step 0 : 0.208643\n",
      "loss in epoch 15 , step 20 : 0.507185\n",
      "loss in epoch 15 , step 40 : 0.146137\n",
      "loss in epoch 15 , step 60 : 0.340133\n",
      "loss in epoch 15 , step 80 : 0.220041\n",
      "loss in epoch 15 , step 100 : 0.279294\n",
      "loss in epoch 15 , step 120 : 0.230467\n",
      "loss in epoch 15 , step 140 : 0.225641\n",
      "loss in epoch 15 , step 160 : 0.255234\n",
      "loss in epoch 15 , step 180 : 0.147918\n",
      "loss in epoch 15 , step 200 : 0.260380\n",
      "loss in epoch 15 , step 220 : 0.209592\n",
      "loss in epoch 15 , step 240 : 0.207825\n",
      "loss in epoch 15 , step 260 : 0.310680\n",
      "loss in epoch 15 , step 280 : 0.368605\n",
      "loss in epoch 15 , step 300 : 0.162531\n",
      "loss in epoch 15 , step 320 : 0.157518\n",
      "loss in epoch 15 , step 340 : 0.188297\n",
      "loss in epoch 15 , step 360 : 0.216481\n",
      "loss in epoch 15 , step 380 : 0.238560\n",
      "loss in epoch 15 , step 400 : 0.191369\n",
      "loss in epoch 15 , step 420 : 0.106005\n",
      "loss in epoch 15 , step 440 : 0.130068\n",
      "loss in epoch 15 , step 460 : 0.290853\n",
      "loss in epoch 15 , step 480 : 0.245638\n",
      "loss in epoch 15 , step 500 : 0.195651\n",
      "loss in epoch 15 , step 520 : 0.224331\n",
      "loss in epoch 15 , step 540 : 0.113174\n",
      "loss in epoch 15 , step 560 : 0.097540\n",
      "loss in epoch 15 , step 580 : 0.264461\n",
      "loss in epoch 15 , step 600 : 0.186059\n",
      "loss in epoch 15 , step 620 : 0.234566\n",
      "loss in epoch 15 , step 640 : 0.067848\n",
      "loss in epoch 15 , step 660 : 0.113086\n",
      "loss in epoch 15 , step 680 : 0.195858\n",
      "loss in epoch 15 , step 700 : 0.094040\n",
      "loss in epoch 15 , step 720 : 0.372878\n",
      "loss in epoch 15 , step 740 : 0.131180\n",
      "loss in epoch 15 , step 760 : 0.161582\n",
      "loss in epoch 15 , step 780 : 0.174503\n",
      "Accuracy in epoch 15 : 90.669998\n",
      "loss in epoch 16 , step 0 : 0.202118\n",
      "loss in epoch 16 , step 20 : 0.102638\n",
      "loss in epoch 16 , step 40 : 0.298720\n",
      "loss in epoch 16 , step 60 : 0.170492\n",
      "loss in epoch 16 , step 80 : 0.164104\n",
      "loss in epoch 16 , step 100 : 0.063084\n",
      "loss in epoch 16 , step 120 : 0.142097\n",
      "loss in epoch 16 , step 140 : 0.330693\n",
      "loss in epoch 16 , step 160 : 0.126663\n",
      "loss in epoch 16 , step 180 : 0.123176\n",
      "loss in epoch 16 , step 200 : 0.121717\n",
      "loss in epoch 16 , step 220 : 0.170720\n",
      "loss in epoch 16 , step 240 : 0.282431\n",
      "loss in epoch 16 , step 260 : 0.165797\n",
      "loss in epoch 16 , step 280 : 0.121023\n",
      "loss in epoch 16 , step 300 : 0.173089\n",
      "loss in epoch 16 , step 320 : 0.210480\n",
      "loss in epoch 16 , step 340 : 0.095391\n",
      "loss in epoch 16 , step 360 : 0.115617\n",
      "loss in epoch 16 , step 380 : 0.087294\n",
      "loss in epoch 16 , step 400 : 0.191772\n",
      "loss in epoch 16 , step 420 : 0.165781\n",
      "loss in epoch 16 , step 440 : 0.167848\n",
      "loss in epoch 16 , step 460 : 0.151873\n",
      "loss in epoch 16 , step 480 : 0.257496\n",
      "loss in epoch 16 , step 500 : 0.217091\n",
      "loss in epoch 16 , step 520 : 0.166637\n",
      "loss in epoch 16 , step 540 : 0.140042\n",
      "loss in epoch 16 , step 560 : 0.158756\n",
      "loss in epoch 16 , step 580 : 0.152935\n",
      "loss in epoch 16 , step 600 : 0.046129\n",
      "loss in epoch 16 , step 620 : 0.196446\n",
      "loss in epoch 16 , step 640 : 0.156402\n",
      "loss in epoch 16 , step 660 : 0.246032\n",
      "loss in epoch 16 , step 680 : 0.239061\n",
      "loss in epoch 16 , step 700 : 0.206995\n",
      "loss in epoch 16 , step 720 : 0.131169\n",
      "loss in epoch 16 , step 740 : 0.145718\n",
      "loss in epoch 16 , step 760 : 0.072909\n",
      "loss in epoch 16 , step 780 : 0.186827\n",
      "Accuracy in epoch 16 : 91.339996\n",
      "loss in epoch 17 , step 0 : 0.117100\n",
      "loss in epoch 17 , step 20 : 0.069370\n",
      "loss in epoch 17 , step 40 : 0.033555\n",
      "loss in epoch 17 , step 60 : 0.254712\n",
      "loss in epoch 17 , step 80 : 0.158223\n",
      "loss in epoch 17 , step 100 : 0.094118\n",
      "loss in epoch 17 , step 120 : 0.107674\n",
      "loss in epoch 17 , step 140 : 0.110145\n",
      "loss in epoch 17 , step 160 : 0.452510\n",
      "loss in epoch 17 , step 180 : 0.187285\n",
      "loss in epoch 17 , step 200 : 0.100498\n",
      "loss in epoch 17 , step 220 : 0.281416\n",
      "loss in epoch 17 , step 240 : 0.103554\n",
      "loss in epoch 17 , step 260 : 0.132518\n",
      "loss in epoch 17 , step 280 : 0.209719\n",
      "loss in epoch 17 , step 300 : 0.068128\n",
      "loss in epoch 17 , step 320 : 0.065091\n",
      "loss in epoch 17 , step 340 : 0.281287\n",
      "loss in epoch 17 , step 360 : 0.202586\n",
      "loss in epoch 17 , step 380 : 0.182960\n",
      "loss in epoch 17 , step 400 : 0.219699\n",
      "loss in epoch 17 , step 420 : 0.169175\n",
      "loss in epoch 17 , step 440 : 0.077135\n",
      "loss in epoch 17 , step 460 : 0.183454\n",
      "loss in epoch 17 , step 480 : 0.090100\n",
      "loss in epoch 17 , step 500 : 0.251346\n",
      "loss in epoch 17 , step 520 : 0.092711\n",
      "loss in epoch 17 , step 540 : 0.148269\n",
      "loss in epoch 17 , step 560 : 0.109527\n",
      "loss in epoch 17 , step 580 : 0.163565\n",
      "loss in epoch 17 , step 600 : 0.076083\n",
      "loss in epoch 17 , step 620 : 0.174766\n",
      "loss in epoch 17 , step 640 : 0.088300\n",
      "loss in epoch 17 , step 660 : 0.246385\n",
      "loss in epoch 17 , step 680 : 0.068931\n",
      "loss in epoch 17 , step 700 : 0.146689\n",
      "loss in epoch 17 , step 720 : 0.199307\n",
      "loss in epoch 17 , step 740 : 0.075388\n",
      "loss in epoch 17 , step 760 : 0.163402\n",
      "loss in epoch 17 , step 780 : 0.147307\n",
      "Accuracy in epoch 17 : 91.230003\n",
      "loss in epoch 18 , step 0 : 0.101449\n",
      "loss in epoch 18 , step 20 : 0.109307\n",
      "loss in epoch 18 , step 40 : 0.096928\n",
      "loss in epoch 18 , step 60 : 0.148931\n",
      "loss in epoch 18 , step 80 : 0.114363\n",
      "loss in epoch 18 , step 100 : 0.130114\n",
      "loss in epoch 18 , step 120 : 0.158234\n",
      "loss in epoch 18 , step 140 : 0.126545\n",
      "loss in epoch 18 , step 160 : 0.139199\n",
      "loss in epoch 18 , step 180 : 0.163591\n",
      "loss in epoch 18 , step 200 : 0.215669\n",
      "loss in epoch 18 , step 220 : 0.144649\n",
      "loss in epoch 18 , step 240 : 0.227254\n",
      "loss in epoch 18 , step 260 : 0.050612\n",
      "loss in epoch 18 , step 280 : 0.199854\n",
      "loss in epoch 18 , step 300 : 0.168803\n",
      "loss in epoch 18 , step 320 : 0.099023\n",
      "loss in epoch 18 , step 340 : 0.110815\n",
      "loss in epoch 18 , step 360 : 0.220290\n",
      "loss in epoch 18 , step 380 : 0.119945\n",
      "loss in epoch 18 , step 400 : 0.162508\n",
      "loss in epoch 18 , step 420 : 0.203344\n",
      "loss in epoch 18 , step 440 : 0.099523\n",
      "loss in epoch 18 , step 460 : 0.170495\n",
      "loss in epoch 18 , step 480 : 0.148271\n",
      "loss in epoch 18 , step 500 : 0.097526\n",
      "loss in epoch 18 , step 520 : 0.177504\n",
      "loss in epoch 18 , step 540 : 0.123332\n",
      "loss in epoch 18 , step 560 : 0.135228\n",
      "loss in epoch 18 , step 580 : 0.120617\n",
      "loss in epoch 18 , step 600 : 0.275167\n",
      "loss in epoch 18 , step 620 : 0.139887\n",
      "loss in epoch 18 , step 640 : 0.179742\n",
      "loss in epoch 18 , step 660 : 0.100064\n",
      "loss in epoch 18 , step 680 : 0.167330\n",
      "loss in epoch 18 , step 700 : 0.249900\n",
      "loss in epoch 18 , step 720 : 0.150777\n",
      "loss in epoch 18 , step 740 : 0.082568\n",
      "loss in epoch 18 , step 760 : 0.185920\n",
      "loss in epoch 18 , step 780 : 0.093715\n",
      "Accuracy in epoch 18 : 91.309998\n",
      "loss in epoch 19 , step 0 : 0.127830\n",
      "loss in epoch 19 , step 20 : 0.144336\n",
      "loss in epoch 19 , step 40 : 0.125607\n",
      "loss in epoch 19 , step 60 : 0.172569\n",
      "loss in epoch 19 , step 80 : 0.066159\n",
      "loss in epoch 19 , step 100 : 0.114859\n",
      "loss in epoch 19 , step 120 : 0.155554\n",
      "loss in epoch 19 , step 140 : 0.088950\n",
      "loss in epoch 19 , step 160 : 0.303396\n",
      "loss in epoch 19 , step 180 : 0.131364\n",
      "loss in epoch 19 , step 200 : 0.146727\n",
      "loss in epoch 19 , step 220 : 0.121614\n",
      "loss in epoch 19 , step 240 : 0.292887\n",
      "loss in epoch 19 , step 260 : 0.173248\n",
      "loss in epoch 19 , step 280 : 0.211821\n",
      "loss in epoch 19 , step 300 : 0.234457\n",
      "loss in epoch 19 , step 320 : 0.216254\n",
      "loss in epoch 19 , step 340 : 0.203473\n",
      "loss in epoch 19 , step 360 : 0.093169\n",
      "loss in epoch 19 , step 380 : 0.066912\n",
      "loss in epoch 19 , step 400 : 0.085113\n",
      "loss in epoch 19 , step 420 : 0.097359\n",
      "loss in epoch 19 , step 440 : 0.123871\n",
      "loss in epoch 19 , step 460 : 0.125490\n",
      "loss in epoch 19 , step 480 : 0.076553\n",
      "loss in epoch 19 , step 500 : 0.100543\n",
      "loss in epoch 19 , step 520 : 0.089339\n",
      "loss in epoch 19 , step 540 : 0.098279\n",
      "loss in epoch 19 , step 560 : 0.129144\n",
      "loss in epoch 19 , step 580 : 0.269529\n",
      "loss in epoch 19 , step 600 : 0.067593\n",
      "loss in epoch 19 , step 620 : 0.204985\n",
      "loss in epoch 19 , step 640 : 0.131564\n",
      "loss in epoch 19 , step 660 : 0.175086\n",
      "loss in epoch 19 , step 680 : 0.160962\n",
      "loss in epoch 19 , step 700 : 0.169951\n",
      "loss in epoch 19 , step 720 : 0.094140\n",
      "loss in epoch 19 , step 740 : 0.071768\n",
      "loss in epoch 19 , step 760 : 0.074418\n",
      "loss in epoch 19 , step 780 : 0.075110\n",
      "Accuracy in epoch 19 : 91.410004\n",
      "loss in epoch 20 , step 0 : 0.074828\n",
      "loss in epoch 20 , step 20 : 0.082239\n",
      "loss in epoch 20 , step 40 : 0.216862\n",
      "loss in epoch 20 , step 60 : 0.148073\n",
      "loss in epoch 20 , step 80 : 0.067814\n",
      "loss in epoch 20 , step 100 : 0.089516\n",
      "loss in epoch 20 , step 120 : 0.168520\n",
      "loss in epoch 20 , step 140 : 0.053532\n",
      "loss in epoch 20 , step 160 : 0.061853\n",
      "loss in epoch 20 , step 180 : 0.199181\n",
      "loss in epoch 20 , step 200 : 0.226661\n",
      "loss in epoch 20 , step 220 : 0.060274\n",
      "loss in epoch 20 , step 240 : 0.165636\n",
      "loss in epoch 20 , step 260 : 0.089068\n",
      "loss in epoch 20 , step 280 : 0.071081\n",
      "loss in epoch 20 , step 300 : 0.238403\n",
      "loss in epoch 20 , step 320 : 0.084365\n",
      "loss in epoch 20 , step 340 : 0.054296\n",
      "loss in epoch 20 , step 360 : 0.100341\n",
      "loss in epoch 20 , step 380 : 0.138710\n",
      "loss in epoch 20 , step 400 : 0.116777\n",
      "loss in epoch 20 , step 420 : 0.121309\n",
      "loss in epoch 20 , step 440 : 0.109256\n",
      "loss in epoch 20 , step 460 : 0.207930\n",
      "loss in epoch 20 , step 480 : 0.173795\n",
      "loss in epoch 20 , step 500 : 0.240555\n",
      "loss in epoch 20 , step 520 : 0.159409\n",
      "loss in epoch 20 , step 540 : 0.105678\n",
      "loss in epoch 20 , step 560 : 0.232692\n",
      "loss in epoch 20 , step 580 : 0.233506\n",
      "loss in epoch 20 , step 600 : 0.162739\n",
      "loss in epoch 20 , step 620 : 0.118408\n",
      "loss in epoch 20 , step 640 : 0.177352\n",
      "loss in epoch 20 , step 660 : 0.068035\n",
      "loss in epoch 20 , step 680 : 0.145088\n",
      "loss in epoch 20 , step 700 : 0.184862\n",
      "loss in epoch 20 , step 720 : 0.354541\n",
      "loss in epoch 20 , step 740 : 0.144203\n",
      "loss in epoch 20 , step 760 : 0.122581\n",
      "loss in epoch 20 , step 780 : 0.191528\n",
      "Accuracy in epoch 20 : 91.660004\n",
      "loss in epoch 21 , step 0 : 0.085191\n",
      "loss in epoch 21 , step 20 : 0.115581\n",
      "loss in epoch 21 , step 40 : 0.055315\n",
      "loss in epoch 21 , step 60 : 0.092749\n",
      "loss in epoch 21 , step 80 : 0.157211\n",
      "loss in epoch 21 , step 100 : 0.083981\n",
      "loss in epoch 21 , step 120 : 0.229053\n",
      "loss in epoch 21 , step 140 : 0.055122\n",
      "loss in epoch 21 , step 160 : 0.118471\n",
      "loss in epoch 21 , step 180 : 0.124505\n",
      "loss in epoch 21 , step 200 : 0.055895\n",
      "loss in epoch 21 , step 220 : 0.100658\n",
      "loss in epoch 21 , step 240 : 0.098204\n",
      "loss in epoch 21 , step 260 : 0.170957\n",
      "loss in epoch 21 , step 280 : 0.109453\n",
      "loss in epoch 21 , step 300 : 0.180324\n",
      "loss in epoch 21 , step 320 : 0.212442\n",
      "loss in epoch 21 , step 340 : 0.114975\n",
      "loss in epoch 21 , step 360 : 0.231550\n",
      "loss in epoch 21 , step 380 : 0.209673\n",
      "loss in epoch 21 , step 400 : 0.075289\n",
      "loss in epoch 21 , step 420 : 0.033318\n",
      "loss in epoch 21 , step 440 : 0.090470\n",
      "loss in epoch 21 , step 460 : 0.068236\n",
      "loss in epoch 21 , step 480 : 0.206756\n",
      "loss in epoch 21 , step 500 : 0.085385\n",
      "loss in epoch 21 , step 520 : 0.034783\n",
      "loss in epoch 21 , step 540 : 0.096001\n",
      "loss in epoch 21 , step 560 : 0.077227\n",
      "loss in epoch 21 , step 580 : 0.050611\n",
      "loss in epoch 21 , step 600 : 0.098798\n",
      "loss in epoch 21 , step 620 : 0.045806\n",
      "loss in epoch 21 , step 640 : 0.135489\n",
      "loss in epoch 21 , step 660 : 0.066800\n",
      "loss in epoch 21 , step 680 : 0.199676\n",
      "loss in epoch 21 , step 700 : 0.119659\n",
      "loss in epoch 21 , step 720 : 0.097481\n",
      "loss in epoch 21 , step 740 : 0.157125\n",
      "loss in epoch 21 , step 760 : 0.175922\n",
      "loss in epoch 21 , step 780 : 0.075609\n",
      "Accuracy in epoch 21 : 91.620003\n",
      "loss in epoch 22 , step 0 : 0.076516\n",
      "loss in epoch 22 , step 20 : 0.110825\n",
      "loss in epoch 22 , step 40 : 0.084787\n",
      "loss in epoch 22 , step 60 : 0.072683\n",
      "loss in epoch 22 , step 80 : 0.094278\n",
      "loss in epoch 22 , step 100 : 0.105177\n",
      "loss in epoch 22 , step 120 : 0.217555\n",
      "loss in epoch 22 , step 140 : 0.104131\n",
      "loss in epoch 22 , step 160 : 0.091514\n",
      "loss in epoch 22 , step 180 : 0.230732\n",
      "loss in epoch 22 , step 200 : 0.215223\n",
      "loss in epoch 22 , step 220 : 0.213800\n",
      "loss in epoch 22 , step 240 : 0.070036\n",
      "loss in epoch 22 , step 260 : 0.101999\n",
      "loss in epoch 22 , step 280 : 0.231740\n",
      "loss in epoch 22 , step 300 : 0.133191\n",
      "loss in epoch 22 , step 320 : 0.142225\n",
      "loss in epoch 22 , step 340 : 0.276996\n",
      "loss in epoch 22 , step 360 : 0.182864\n",
      "loss in epoch 22 , step 380 : 0.226221\n",
      "loss in epoch 22 , step 400 : 0.057901\n",
      "loss in epoch 22 , step 420 : 0.105823\n",
      "loss in epoch 22 , step 440 : 0.085028\n",
      "loss in epoch 22 , step 460 : 0.027141\n",
      "loss in epoch 22 , step 480 : 0.036078\n",
      "loss in epoch 22 , step 500 : 0.098011\n",
      "loss in epoch 22 , step 520 : 0.105536\n",
      "loss in epoch 22 , step 540 : 0.047790\n",
      "loss in epoch 22 , step 560 : 0.169642\n",
      "loss in epoch 22 , step 580 : 0.089021\n",
      "loss in epoch 22 , step 600 : 0.110655\n",
      "loss in epoch 22 , step 620 : 0.214977\n",
      "loss in epoch 22 , step 640 : 0.092043\n",
      "loss in epoch 22 , step 660 : 0.117202\n",
      "loss in epoch 22 , step 680 : 0.075674\n",
      "loss in epoch 22 , step 700 : 0.089936\n",
      "loss in epoch 22 , step 720 : 0.221418\n",
      "loss in epoch 22 , step 740 : 0.076731\n",
      "loss in epoch 22 , step 760 : 0.064534\n",
      "loss in epoch 22 , step 780 : 0.100876\n",
      "Accuracy in epoch 22 : 91.809998\n",
      "loss in epoch 23 , step 0 : 0.121173\n",
      "loss in epoch 23 , step 20 : 0.068237\n",
      "loss in epoch 23 , step 40 : 0.065340\n",
      "loss in epoch 23 , step 60 : 0.070306\n",
      "loss in epoch 23 , step 80 : 0.113813\n",
      "loss in epoch 23 , step 100 : 0.090386\n",
      "loss in epoch 23 , step 120 : 0.073882\n",
      "loss in epoch 23 , step 140 : 0.157193\n",
      "loss in epoch 23 , step 160 : 0.140108\n",
      "loss in epoch 23 , step 180 : 0.057820\n",
      "loss in epoch 23 , step 200 : 0.123142\n",
      "loss in epoch 23 , step 220 : 0.089034\n",
      "loss in epoch 23 , step 240 : 0.189870\n",
      "loss in epoch 23 , step 260 : 0.144867\n",
      "loss in epoch 23 , step 280 : 0.030095\n",
      "loss in epoch 23 , step 300 : 0.152723\n",
      "loss in epoch 23 , step 320 : 0.033420\n",
      "loss in epoch 23 , step 340 : 0.121177\n",
      "loss in epoch 23 , step 360 : 0.087706\n",
      "loss in epoch 23 , step 380 : 0.117679\n",
      "loss in epoch 23 , step 400 : 0.100808\n",
      "loss in epoch 23 , step 420 : 0.109853\n",
      "loss in epoch 23 , step 440 : 0.146065\n",
      "loss in epoch 23 , step 460 : 0.081353\n",
      "loss in epoch 23 , step 480 : 0.049201\n",
      "loss in epoch 23 , step 500 : 0.052778\n",
      "loss in epoch 23 , step 520 : 0.179818\n",
      "loss in epoch 23 , step 540 : 0.113266\n",
      "loss in epoch 23 , step 560 : 0.087550\n",
      "loss in epoch 23 , step 580 : 0.113787\n",
      "loss in epoch 23 , step 600 : 0.130472\n",
      "loss in epoch 23 , step 620 : 0.199588\n",
      "loss in epoch 23 , step 640 : 0.171521\n",
      "loss in epoch 23 , step 660 : 0.147170\n",
      "loss in epoch 23 , step 680 : 0.165966\n",
      "loss in epoch 23 , step 700 : 0.181642\n",
      "loss in epoch 23 , step 720 : 0.042105\n",
      "loss in epoch 23 , step 740 : 0.070993\n",
      "loss in epoch 23 , step 760 : 0.145145\n",
      "loss in epoch 23 , step 780 : 0.041460\n",
      "Accuracy in epoch 23 : 91.739998\n",
      "loss in epoch 24 , step 0 : 0.160182\n",
      "loss in epoch 24 , step 20 : 0.100646\n",
      "loss in epoch 24 , step 40 : 0.124606\n",
      "loss in epoch 24 , step 60 : 0.273030\n",
      "loss in epoch 24 , step 80 : 0.130520\n",
      "loss in epoch 24 , step 100 : 0.112478\n",
      "loss in epoch 24 , step 120 : 0.224986\n",
      "loss in epoch 24 , step 140 : 0.083476\n",
      "loss in epoch 24 , step 160 : 0.175703\n",
      "loss in epoch 24 , step 180 : 0.139221\n",
      "loss in epoch 24 , step 200 : 0.055745\n",
      "loss in epoch 24 , step 220 : 0.054978\n",
      "loss in epoch 24 , step 240 : 0.076927\n",
      "loss in epoch 24 , step 260 : 0.063507\n",
      "loss in epoch 24 , step 280 : 0.039837\n",
      "loss in epoch 24 , step 300 : 0.027131\n",
      "loss in epoch 24 , step 320 : 0.045989\n",
      "loss in epoch 24 , step 340 : 0.064678\n",
      "loss in epoch 24 , step 360 : 0.082296\n",
      "loss in epoch 24 , step 380 : 0.068203\n",
      "loss in epoch 24 , step 400 : 0.053258\n",
      "loss in epoch 24 , step 420 : 0.062048\n",
      "loss in epoch 24 , step 440 : 0.134742\n",
      "loss in epoch 24 , step 460 : 0.202216\n",
      "loss in epoch 24 , step 480 : 0.086758\n",
      "loss in epoch 24 , step 500 : 0.098966\n",
      "loss in epoch 24 , step 520 : 0.072313\n",
      "loss in epoch 24 , step 540 : 0.072570\n",
      "loss in epoch 24 , step 560 : 0.110976\n",
      "loss in epoch 24 , step 580 : 0.038108\n",
      "loss in epoch 24 , step 600 : 0.162748\n",
      "loss in epoch 24 , step 620 : 0.056331\n",
      "loss in epoch 24 , step 640 : 0.075247\n",
      "loss in epoch 24 , step 660 : 0.056552\n",
      "loss in epoch 24 , step 680 : 0.071341\n",
      "loss in epoch 24 , step 700 : 0.092559\n",
      "loss in epoch 24 , step 720 : 0.118282\n",
      "loss in epoch 24 , step 740 : 0.080399\n",
      "loss in epoch 24 , step 760 : 0.142723\n",
      "loss in epoch 24 , step 780 : 0.099923\n",
      "Accuracy in epoch 24 : 91.400002\n",
      "loss in epoch 25 , step 0 : 0.126039\n",
      "loss in epoch 25 , step 20 : 0.059368\n",
      "loss in epoch 25 , step 40 : 0.078147\n",
      "loss in epoch 25 , step 60 : 0.106356\n",
      "loss in epoch 25 , step 80 : 0.088045\n",
      "loss in epoch 25 , step 100 : 0.075280\n",
      "loss in epoch 25 , step 120 : 0.046456\n",
      "loss in epoch 25 , step 140 : 0.144830\n",
      "loss in epoch 25 , step 160 : 0.060972\n",
      "loss in epoch 25 , step 180 : 0.035585\n",
      "loss in epoch 25 , step 200 : 0.127544\n",
      "loss in epoch 25 , step 220 : 0.047798\n",
      "loss in epoch 25 , step 240 : 0.045145\n",
      "loss in epoch 25 , step 260 : 0.100540\n",
      "loss in epoch 25 , step 280 : 0.082518\n",
      "loss in epoch 25 , step 300 : 0.086304\n",
      "loss in epoch 25 , step 320 : 0.087445\n",
      "loss in epoch 25 , step 340 : 0.203580\n",
      "loss in epoch 25 , step 360 : 0.058683\n",
      "loss in epoch 25 , step 380 : 0.082783\n",
      "loss in epoch 25 , step 400 : 0.071139\n",
      "loss in epoch 25 , step 420 : 0.119868\n",
      "loss in epoch 25 , step 440 : 0.027829\n",
      "loss in epoch 25 , step 460 : 0.041937\n",
      "loss in epoch 25 , step 480 : 0.069813\n",
      "loss in epoch 25 , step 500 : 0.195946\n",
      "loss in epoch 25 , step 520 : 0.136917\n",
      "loss in epoch 25 , step 540 : 0.126375\n",
      "loss in epoch 25 , step 560 : 0.080578\n",
      "loss in epoch 25 , step 580 : 0.089452\n",
      "loss in epoch 25 , step 600 : 0.025088\n",
      "loss in epoch 25 , step 620 : 0.064286\n",
      "loss in epoch 25 , step 640 : 0.078842\n",
      "loss in epoch 25 , step 660 : 0.039908\n",
      "loss in epoch 25 , step 680 : 0.059506\n",
      "loss in epoch 25 , step 700 : 0.206797\n",
      "loss in epoch 25 , step 720 : 0.129464\n",
      "loss in epoch 25 , step 740 : 0.216061\n",
      "loss in epoch 25 , step 760 : 0.132121\n",
      "loss in epoch 25 , step 780 : 0.086901\n",
      "Accuracy in epoch 25 : 91.709999\n",
      "loss in epoch 26 , step 0 : 0.081296\n",
      "loss in epoch 26 , step 20 : 0.057729\n",
      "loss in epoch 26 , step 40 : 0.072860\n",
      "loss in epoch 26 , step 60 : 0.084021\n",
      "loss in epoch 26 , step 80 : 0.044805\n",
      "loss in epoch 26 , step 100 : 0.067942\n",
      "loss in epoch 26 , step 120 : 0.087636\n",
      "loss in epoch 26 , step 140 : 0.135088\n",
      "loss in epoch 26 , step 160 : 0.078710\n",
      "loss in epoch 26 , step 180 : 0.057106\n",
      "loss in epoch 26 , step 200 : 0.043962\n",
      "loss in epoch 26 , step 220 : 0.078926\n",
      "loss in epoch 26 , step 240 : 0.117876\n",
      "loss in epoch 26 , step 260 : 0.195893\n",
      "loss in epoch 26 , step 280 : 0.100758\n",
      "loss in epoch 26 , step 300 : 0.080318\n",
      "loss in epoch 26 , step 320 : 0.121201\n",
      "loss in epoch 26 , step 340 : 0.057138\n",
      "loss in epoch 26 , step 360 : 0.065577\n",
      "loss in epoch 26 , step 380 : 0.211196\n",
      "loss in epoch 26 , step 400 : 0.112987\n",
      "loss in epoch 26 , step 420 : 0.069350\n",
      "loss in epoch 26 , step 440 : 0.036291\n",
      "loss in epoch 26 , step 460 : 0.078941\n",
      "loss in epoch 26 , step 480 : 0.043375\n",
      "loss in epoch 26 , step 500 : 0.072261\n",
      "loss in epoch 26 , step 520 : 0.052480\n",
      "loss in epoch 26 , step 540 : 0.068001\n",
      "loss in epoch 26 , step 560 : 0.082181\n",
      "loss in epoch 26 , step 580 : 0.176422\n",
      "loss in epoch 26 , step 600 : 0.062517\n",
      "loss in epoch 26 , step 620 : 0.088736\n",
      "loss in epoch 26 , step 640 : 0.016972\n",
      "loss in epoch 26 , step 660 : 0.110827\n",
      "loss in epoch 26 , step 680 : 0.083982\n",
      "loss in epoch 26 , step 700 : 0.118226\n",
      "loss in epoch 26 , step 720 : 0.046746\n",
      "loss in epoch 26 , step 740 : 0.155383\n",
      "loss in epoch 26 , step 760 : 0.198257\n",
      "loss in epoch 26 , step 780 : 0.083731\n",
      "Accuracy in epoch 26 : 91.320000\n",
      "loss in epoch 27 , step 0 : 0.054992\n",
      "loss in epoch 27 , step 20 : 0.124511\n",
      "loss in epoch 27 , step 40 : 0.068175\n",
      "loss in epoch 27 , step 60 : 0.044719\n",
      "loss in epoch 27 , step 80 : 0.216672\n",
      "loss in epoch 27 , step 100 : 0.067342\n",
      "loss in epoch 27 , step 120 : 0.052078\n",
      "loss in epoch 27 , step 140 : 0.071513\n",
      "loss in epoch 27 , step 160 : 0.082749\n",
      "loss in epoch 27 , step 180 : 0.099948\n",
      "loss in epoch 27 , step 200 : 0.090684\n",
      "loss in epoch 27 , step 220 : 0.059627\n",
      "loss in epoch 27 , step 240 : 0.040669\n",
      "loss in epoch 27 , step 260 : 0.038122\n",
      "loss in epoch 27 , step 280 : 0.026296\n",
      "loss in epoch 27 , step 300 : 0.116174\n",
      "loss in epoch 27 , step 320 : 0.048304\n",
      "loss in epoch 27 , step 340 : 0.105757\n",
      "loss in epoch 27 , step 360 : 0.049422\n",
      "loss in epoch 27 , step 380 : 0.050742\n",
      "loss in epoch 27 , step 400 : 0.085213\n",
      "loss in epoch 27 , step 420 : 0.073791\n",
      "loss in epoch 27 , step 440 : 0.046048\n",
      "loss in epoch 27 , step 460 : 0.060306\n",
      "loss in epoch 27 , step 480 : 0.064521\n",
      "loss in epoch 27 , step 500 : 0.093300\n",
      "loss in epoch 27 , step 520 : 0.074499\n",
      "loss in epoch 27 , step 540 : 0.095804\n",
      "loss in epoch 27 , step 560 : 0.122736\n",
      "loss in epoch 27 , step 580 : 0.029159\n",
      "loss in epoch 27 , step 600 : 0.166895\n",
      "loss in epoch 27 , step 620 : 0.076920\n",
      "loss in epoch 27 , step 640 : 0.137829\n",
      "loss in epoch 27 , step 660 : 0.096373\n",
      "loss in epoch 27 , step 680 : 0.081977\n",
      "loss in epoch 27 , step 700 : 0.149964\n",
      "loss in epoch 27 , step 720 : 0.051071\n",
      "loss in epoch 27 , step 740 : 0.055896\n",
      "loss in epoch 27 , step 760 : 0.122587\n",
      "loss in epoch 27 , step 780 : 0.095163\n",
      "Accuracy in epoch 27 : 91.370003\n",
      "loss in epoch 28 , step 0 : 0.084945\n",
      "loss in epoch 28 , step 20 : 0.050230\n",
      "loss in epoch 28 , step 40 : 0.055042\n",
      "loss in epoch 28 , step 60 : 0.089167\n",
      "loss in epoch 28 , step 80 : 0.044518\n",
      "loss in epoch 28 , step 100 : 0.133745\n",
      "loss in epoch 28 , step 120 : 0.073416\n",
      "loss in epoch 28 , step 140 : 0.025878\n",
      "loss in epoch 28 , step 160 : 0.029887\n",
      "loss in epoch 28 , step 180 : 0.033918\n",
      "loss in epoch 28 , step 200 : 0.179232\n",
      "loss in epoch 28 , step 220 : 0.052613\n",
      "loss in epoch 28 , step 240 : 0.079851\n",
      "loss in epoch 28 , step 260 : 0.111689\n",
      "loss in epoch 28 , step 280 : 0.054644\n",
      "loss in epoch 28 , step 300 : 0.056752\n",
      "loss in epoch 28 , step 320 : 0.096924\n",
      "loss in epoch 28 , step 340 : 0.056715\n",
      "loss in epoch 28 , step 360 : 0.070755\n",
      "loss in epoch 28 , step 380 : 0.059674\n",
      "loss in epoch 28 , step 400 : 0.091427\n",
      "loss in epoch 28 , step 420 : 0.116986\n",
      "loss in epoch 28 , step 440 : 0.104168\n",
      "loss in epoch 28 , step 460 : 0.111949\n",
      "loss in epoch 28 , step 480 : 0.169375\n",
      "loss in epoch 28 , step 500 : 0.243164\n",
      "loss in epoch 28 , step 520 : 0.138646\n",
      "loss in epoch 28 , step 540 : 0.069749\n",
      "loss in epoch 28 , step 560 : 0.082483\n",
      "loss in epoch 28 , step 580 : 0.061403\n",
      "loss in epoch 28 , step 600 : 0.094370\n",
      "loss in epoch 28 , step 620 : 0.108043\n",
      "loss in epoch 28 , step 640 : 0.088525\n",
      "loss in epoch 28 , step 660 : 0.027154\n",
      "loss in epoch 28 , step 680 : 0.032033\n",
      "loss in epoch 28 , step 700 : 0.034198\n",
      "loss in epoch 28 , step 720 : 0.070622\n",
      "loss in epoch 28 , step 740 : 0.171622\n",
      "loss in epoch 28 , step 760 : 0.174504\n",
      "loss in epoch 28 , step 780 : 0.086055\n",
      "Accuracy in epoch 28 : 91.849998\n",
      "loss in epoch 29 , step 0 : 0.075588\n",
      "loss in epoch 29 , step 20 : 0.165469\n",
      "loss in epoch 29 , step 40 : 0.103791\n",
      "loss in epoch 29 , step 60 : 0.032331\n",
      "loss in epoch 29 , step 80 : 0.075741\n",
      "loss in epoch 29 , step 100 : 0.085171\n",
      "loss in epoch 29 , step 120 : 0.044965\n",
      "loss in epoch 29 , step 140 : 0.072688\n",
      "loss in epoch 29 , step 160 : 0.099281\n",
      "loss in epoch 29 , step 180 : 0.124152\n",
      "loss in epoch 29 , step 200 : 0.016031\n",
      "loss in epoch 29 , step 220 : 0.067409\n",
      "loss in epoch 29 , step 240 : 0.043297\n",
      "loss in epoch 29 , step 260 : 0.067647\n",
      "loss in epoch 29 , step 280 : 0.073280\n",
      "loss in epoch 29 , step 300 : 0.110808\n",
      "loss in epoch 29 , step 320 : 0.043887\n",
      "loss in epoch 29 , step 340 : 0.166255\n",
      "loss in epoch 29 , step 360 : 0.073114\n",
      "loss in epoch 29 , step 380 : 0.149553\n",
      "loss in epoch 29 , step 400 : 0.081282\n",
      "loss in epoch 29 , step 420 : 0.061373\n",
      "loss in epoch 29 , step 440 : 0.094600\n",
      "loss in epoch 29 , step 460 : 0.058316\n",
      "loss in epoch 29 , step 480 : 0.124042\n",
      "loss in epoch 29 , step 500 : 0.094643\n",
      "loss in epoch 29 , step 520 : 0.141296\n",
      "loss in epoch 29 , step 540 : 0.103630\n",
      "loss in epoch 29 , step 560 : 0.059149\n",
      "loss in epoch 29 , step 580 : 0.102789\n",
      "loss in epoch 29 , step 600 : 0.101742\n",
      "loss in epoch 29 , step 620 : 0.013552\n",
      "loss in epoch 29 , step 640 : 0.079247\n",
      "loss in epoch 29 , step 660 : 0.055948\n",
      "loss in epoch 29 , step 680 : 0.122310\n",
      "loss in epoch 29 , step 700 : 0.056975\n",
      "loss in epoch 29 , step 720 : 0.112922\n",
      "loss in epoch 29 , step 740 : 0.049442\n",
      "loss in epoch 29 , step 760 : 0.120325\n",
      "loss in epoch 29 , step 780 : 0.029704\n",
      "Accuracy in epoch 29 : 91.529999\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,30):\n",
    "    adjust_lr(optimizer,epoch,learning_rate)\n",
    "    train(train_loader,model,criterion,optimizer,epoch)\n",
    "    test(test_loader,model,criterion,epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6nVj4Pkt7T3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DenseNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1e54d995502743a9a408c72edabec8a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ae2fb781a0104186b4c35c31ccf2aae6",
       "IPY_MODEL_b6ea62a6adc040ae84a30649407d9e54"
      ],
      "layout": "IPY_MODEL_5df2b2cab2b247cf9287e90355bcd2fd"
     }
    },
    "4a8422957ef34c1f9405d311b48f6217": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5df2b2cab2b247cf9287e90355bcd2fd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98782d9738814ce18207039b233f8e53": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab61a4b585334603860d89cdad0c274f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae2fb781a0104186b4c35c31ccf2aae6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a8422957ef34c1f9405d311b48f6217",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b06c87ca860b41628522f9f3d829ece3",
      "value": 1
     }
    },
    "b06c87ca860b41628522f9f3d829ece3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b6ea62a6adc040ae84a30649407d9e54": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab61a4b585334603860d89cdad0c274f",
      "placeholder": "​",
      "style": "IPY_MODEL_98782d9738814ce18207039b233f8e53",
      "value": " 170500096/? [00:20&lt;00:00, 51161457.83it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
